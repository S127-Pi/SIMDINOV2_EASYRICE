{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = torch.load(\"patch2/eval/training_9999/teacher_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['backbone.cls_token',\n",
       "  'backbone.pos_embed',\n",
       "  'backbone.register_tokens',\n",
       "  'backbone.mask_token',\n",
       "  'backbone.patch_embed.proj.weight'],\n",
       " 182)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys_my_model = list(model_state_dict['teacher'])\n",
    "keys_my_model[:5], len(keys_my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_state_dict = torch.load(\"pretrained_weights/dinov2_vits14_pretrain.pth\")\n",
    "pretrained_model_state_dict = torch.load(\"pretrained_weights/dinov2_vits14_reg4_pretrain.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['cls_token',\n",
       "  'register_tokens',\n",
       "  'pos_embed',\n",
       "  'mask_token',\n",
       "  'patch_embed.proj.weight',\n",
       "  'patch_embed.proj.bias',\n",
       "  'blocks.0.norm1.weight',\n",
       "  'blocks.0.norm1.bias',\n",
       "  'blocks.0.attn.qkv.weight',\n",
       "  'blocks.0.attn.qkv.bias',\n",
       "  'blocks.0.attn.proj.weight',\n",
       "  'blocks.0.attn.proj.bias',\n",
       "  'blocks.0.ls1.gamma',\n",
       "  'blocks.0.norm2.weight',\n",
       "  'blocks.0.norm2.bias',\n",
       "  'blocks.0.mlp.fc1.weight',\n",
       "  'blocks.0.mlp.fc1.bias',\n",
       "  'blocks.0.mlp.fc2.weight',\n",
       "  'blocks.0.mlp.fc2.bias',\n",
       "  'blocks.0.ls2.gamma',\n",
       "  'blocks.1.norm1.weight',\n",
       "  'blocks.1.norm1.bias',\n",
       "  'blocks.1.attn.qkv.weight',\n",
       "  'blocks.1.attn.qkv.bias',\n",
       "  'blocks.1.attn.proj.weight',\n",
       "  'blocks.1.attn.proj.bias',\n",
       "  'blocks.1.ls1.gamma',\n",
       "  'blocks.1.norm2.weight',\n",
       "  'blocks.1.norm2.bias',\n",
       "  'blocks.1.mlp.fc1.weight',\n",
       "  'blocks.1.mlp.fc1.bias',\n",
       "  'blocks.1.mlp.fc2.weight',\n",
       "  'blocks.1.mlp.fc2.bias',\n",
       "  'blocks.1.ls2.gamma',\n",
       "  'blocks.2.norm1.weight',\n",
       "  'blocks.2.norm1.bias',\n",
       "  'blocks.2.attn.qkv.weight',\n",
       "  'blocks.2.attn.qkv.bias',\n",
       "  'blocks.2.attn.proj.weight',\n",
       "  'blocks.2.attn.proj.bias',\n",
       "  'blocks.2.ls1.gamma',\n",
       "  'blocks.2.norm2.weight',\n",
       "  'blocks.2.norm2.bias',\n",
       "  'blocks.2.mlp.fc1.weight',\n",
       "  'blocks.2.mlp.fc1.bias',\n",
       "  'blocks.2.mlp.fc2.weight',\n",
       "  'blocks.2.mlp.fc2.bias',\n",
       "  'blocks.2.ls2.gamma',\n",
       "  'blocks.3.norm1.weight',\n",
       "  'blocks.3.norm1.bias',\n",
       "  'blocks.3.attn.qkv.weight',\n",
       "  'blocks.3.attn.qkv.bias',\n",
       "  'blocks.3.attn.proj.weight',\n",
       "  'blocks.3.attn.proj.bias',\n",
       "  'blocks.3.ls1.gamma',\n",
       "  'blocks.3.norm2.weight',\n",
       "  'blocks.3.norm2.bias',\n",
       "  'blocks.3.mlp.fc1.weight',\n",
       "  'blocks.3.mlp.fc1.bias',\n",
       "  'blocks.3.mlp.fc2.weight',\n",
       "  'blocks.3.mlp.fc2.bias',\n",
       "  'blocks.3.ls2.gamma',\n",
       "  'blocks.4.norm1.weight',\n",
       "  'blocks.4.norm1.bias',\n",
       "  'blocks.4.attn.qkv.weight',\n",
       "  'blocks.4.attn.qkv.bias',\n",
       "  'blocks.4.attn.proj.weight',\n",
       "  'blocks.4.attn.proj.bias',\n",
       "  'blocks.4.ls1.gamma',\n",
       "  'blocks.4.norm2.weight',\n",
       "  'blocks.4.norm2.bias',\n",
       "  'blocks.4.mlp.fc1.weight',\n",
       "  'blocks.4.mlp.fc1.bias',\n",
       "  'blocks.4.mlp.fc2.weight',\n",
       "  'blocks.4.mlp.fc2.bias',\n",
       "  'blocks.4.ls2.gamma',\n",
       "  'blocks.5.norm1.weight',\n",
       "  'blocks.5.norm1.bias',\n",
       "  'blocks.5.attn.qkv.weight',\n",
       "  'blocks.5.attn.qkv.bias',\n",
       "  'blocks.5.attn.proj.weight',\n",
       "  'blocks.5.attn.proj.bias',\n",
       "  'blocks.5.ls1.gamma',\n",
       "  'blocks.5.norm2.weight',\n",
       "  'blocks.5.norm2.bias',\n",
       "  'blocks.5.mlp.fc1.weight',\n",
       "  'blocks.5.mlp.fc1.bias',\n",
       "  'blocks.5.mlp.fc2.weight',\n",
       "  'blocks.5.mlp.fc2.bias',\n",
       "  'blocks.5.ls2.gamma',\n",
       "  'blocks.6.norm1.weight',\n",
       "  'blocks.6.norm1.bias',\n",
       "  'blocks.6.attn.qkv.weight',\n",
       "  'blocks.6.attn.qkv.bias',\n",
       "  'blocks.6.attn.proj.weight',\n",
       "  'blocks.6.attn.proj.bias',\n",
       "  'blocks.6.ls1.gamma',\n",
       "  'blocks.6.norm2.weight',\n",
       "  'blocks.6.norm2.bias',\n",
       "  'blocks.6.mlp.fc1.weight',\n",
       "  'blocks.6.mlp.fc1.bias',\n",
       "  'blocks.6.mlp.fc2.weight',\n",
       "  'blocks.6.mlp.fc2.bias',\n",
       "  'blocks.6.ls2.gamma',\n",
       "  'blocks.7.norm1.weight',\n",
       "  'blocks.7.norm1.bias',\n",
       "  'blocks.7.attn.qkv.weight',\n",
       "  'blocks.7.attn.qkv.bias',\n",
       "  'blocks.7.attn.proj.weight',\n",
       "  'blocks.7.attn.proj.bias',\n",
       "  'blocks.7.ls1.gamma',\n",
       "  'blocks.7.norm2.weight',\n",
       "  'blocks.7.norm2.bias',\n",
       "  'blocks.7.mlp.fc1.weight',\n",
       "  'blocks.7.mlp.fc1.bias',\n",
       "  'blocks.7.mlp.fc2.weight',\n",
       "  'blocks.7.mlp.fc2.bias',\n",
       "  'blocks.7.ls2.gamma',\n",
       "  'blocks.8.norm1.weight',\n",
       "  'blocks.8.norm1.bias',\n",
       "  'blocks.8.attn.qkv.weight',\n",
       "  'blocks.8.attn.qkv.bias',\n",
       "  'blocks.8.attn.proj.weight',\n",
       "  'blocks.8.attn.proj.bias',\n",
       "  'blocks.8.ls1.gamma',\n",
       "  'blocks.8.norm2.weight',\n",
       "  'blocks.8.norm2.bias',\n",
       "  'blocks.8.mlp.fc1.weight',\n",
       "  'blocks.8.mlp.fc1.bias',\n",
       "  'blocks.8.mlp.fc2.weight',\n",
       "  'blocks.8.mlp.fc2.bias',\n",
       "  'blocks.8.ls2.gamma',\n",
       "  'blocks.9.norm1.weight',\n",
       "  'blocks.9.norm1.bias',\n",
       "  'blocks.9.attn.qkv.weight',\n",
       "  'blocks.9.attn.qkv.bias',\n",
       "  'blocks.9.attn.proj.weight',\n",
       "  'blocks.9.attn.proj.bias',\n",
       "  'blocks.9.ls1.gamma',\n",
       "  'blocks.9.norm2.weight',\n",
       "  'blocks.9.norm2.bias',\n",
       "  'blocks.9.mlp.fc1.weight',\n",
       "  'blocks.9.mlp.fc1.bias',\n",
       "  'blocks.9.mlp.fc2.weight',\n",
       "  'blocks.9.mlp.fc2.bias',\n",
       "  'blocks.9.ls2.gamma',\n",
       "  'blocks.10.norm1.weight',\n",
       "  'blocks.10.norm1.bias',\n",
       "  'blocks.10.attn.qkv.weight',\n",
       "  'blocks.10.attn.qkv.bias',\n",
       "  'blocks.10.attn.proj.weight',\n",
       "  'blocks.10.attn.proj.bias',\n",
       "  'blocks.10.ls1.gamma',\n",
       "  'blocks.10.norm2.weight',\n",
       "  'blocks.10.norm2.bias',\n",
       "  'blocks.10.mlp.fc1.weight',\n",
       "  'blocks.10.mlp.fc1.bias',\n",
       "  'blocks.10.mlp.fc2.weight',\n",
       "  'blocks.10.mlp.fc2.bias',\n",
       "  'blocks.10.ls2.gamma',\n",
       "  'blocks.11.norm1.weight',\n",
       "  'blocks.11.norm1.bias',\n",
       "  'blocks.11.attn.qkv.weight',\n",
       "  'blocks.11.attn.qkv.bias',\n",
       "  'blocks.11.attn.proj.weight',\n",
       "  'blocks.11.attn.proj.bias',\n",
       "  'blocks.11.ls1.gamma',\n",
       "  'blocks.11.norm2.weight',\n",
       "  'blocks.11.norm2.bias',\n",
       "  'blocks.11.mlp.fc1.weight',\n",
       "  'blocks.11.mlp.fc1.bias',\n",
       "  'blocks.11.mlp.fc2.weight',\n",
       "  'blocks.11.mlp.fc2.bias',\n",
       "  'blocks.11.ls2.gamma',\n",
       "  'norm.weight',\n",
       "  'norm.bias'],\n",
       " 176)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys_pretrained_model = list(pretrained_model_state_dict.keys())\n",
    "keys_pretrained_model[:], len(keys_pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 384])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_state_dict['register_tokens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure each 0-3 prefix is repeated 42 times\n",
    "prefixes = [\"0\", \"1\", \"2\", \"3\"]\n",
    "\n",
    "# Example state_dict keys\n",
    "state_dict_keys = {\n",
    "    \"dino_head.last_layer.weight_v'\": 1,\n",
    "    \"teacher.backbone.blocks.3.attn.qkv.bias\": 0,\n",
    "    \"teacher.backbone.blocks.1.mlp.fc2.weight\": 1,\n",
    "    \"teacher.backbone.blocks.2.ls2.gamma\": 0\n",
    "}\n",
    "\n",
    "# Pattern to extract the common part \"teacher.backbone.blocks.\"\n",
    "pattern = re.compile(r\"(teacher\\.backbone\\.blocks\\.)\")\n",
    "\n",
    "# Transform keys\n",
    "new_keys = []\n",
    "counter = 0\n",
    "idx = 0\n",
    "# Iterate through the state_dict keys\n",
    "for key, value in state_dict_keys.items():\n",
    "    match = pattern.match(key)  # Match the \"teacher.backbone.blocks.\"\n",
    "    \n",
    "    if match:\n",
    "        # Extract the common part of the key before the digit\n",
    "        common_part = match.group(1)  # This will give us \"teacher.backbone.blocks.\"\n",
    "        \n",
    "        # Add suffixes and append the remaining part of the key\n",
    "        for prefix in prefixes[idx]:\n",
    "            new_key = f\"{common_part}{prefix}.{key[len(common_part):]}\"  # Add the suffix and reattach the rest\n",
    "            new_keys.append(new_key)\n",
    "        counter += 1  # Move to the next prefix\n",
    "        if counter % 42 == 0:\n",
    "            idx += 1\n",
    "\n",
    "    else:\n",
    "        new_keys.append(key)  # Keep unchanged if no match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeat_prefix = ['backbone.blocks.0.0.norm1.weight',\n",
    "  'backbone.blocks.0.0.norm1.bias',\n",
    "  'backbone.blocks.0.0.attn.qkv.weight',\n",
    "  'backbone.blocks.0.0.attn.qkv.bias',\n",
    "  'backbone.blocks.0.0.attn.proj.weight',\n",
    "  'backbone.blocks.0.0.attn.proj.bias',\n",
    "  'backbone.blocks.0.0.ls1.gamma',\n",
    "  'backbone.blocks.0.0.norm2.weight',\n",
    "  'backbone.blocks.0.0.norm2.bias',\n",
    "  'backbone.blocks.0.0.mlp.fc1.weight',\n",
    "  'backbone.blocks.0.0.mlp.fc1.bias',\n",
    "  'backbone.blocks.0.0.mlp.fc2.weight',\n",
    "  'backbone.blocks.0.0.mlp.fc2.bias',\n",
    "  'backbone.blocks.0.0.ls2.gamma',\n",
    "  'backbone.blocks.0.1.norm1.weight',\n",
    "  'backbone.blocks.0.1.norm1.bias',\n",
    "  'backbone.blocks.0.1.attn.qkv.weight',\n",
    "  'backbone.blocks.0.1.attn.qkv.bias',\n",
    "  'backbone.blocks.0.1.attn.proj.weight',\n",
    "  'backbone.blocks.0.1.attn.proj.bias',\n",
    "  'backbone.blocks.0.1.ls1.gamma',\n",
    "  'backbone.blocks.0.1.norm2.weight',\n",
    "  'backbone.blocks.0.1.norm2.bias',\n",
    "  'backbone.blocks.0.1.mlp.fc1.weight',\n",
    "  'backbone.blocks.0.1.mlp.fc1.bias',\n",
    "  'backbone.blocks.0.1.mlp.fc2.weight',\n",
    "  'backbone.blocks.0.1.mlp.fc2.bias',\n",
    "  'backbone.blocks.0.1.ls2.gamma',\n",
    "  'backbone.blocks.0.2.norm1.weight',\n",
    "  'backbone.blocks.0.2.norm1.bias',\n",
    "  'backbone.blocks.0.2.attn.qkv.weight',\n",
    "  'backbone.blocks.0.2.attn.qkv.bias',\n",
    "  'backbone.blocks.0.2.attn.proj.weight',\n",
    "  'backbone.blocks.0.2.attn.proj.bias',\n",
    "  'backbone.blocks.0.2.ls1.gamma',\n",
    "  'backbone.blocks.0.2.norm2.weight',\n",
    "  'backbone.blocks.0.2.norm2.bias',\n",
    "  'backbone.blocks.0.2.mlp.fc1.weight',\n",
    "  'backbone.blocks.0.2.mlp.fc1.bias',\n",
    "  'backbone.blocks.0.2.mlp.fc2.weight',\n",
    "  'backbone.blocks.0.2.mlp.fc2.bias',\n",
    "  'backbone.blocks.0.2.ls2.gamma']\n",
    "\n",
    "len(repeat_prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
